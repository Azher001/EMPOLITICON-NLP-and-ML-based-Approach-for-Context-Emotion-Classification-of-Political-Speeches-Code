{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1483d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d773eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac02eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset_Before_Preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbde03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not yet... this will be updated accordingly\n",
    "nltk_data_path = os.popen('which python').read().replace('bin/python\\n', '').strip() + 'nltk_data/'\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66287235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT -- TRAINING CONFIGURATION ONLY #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf73165",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data PreProcessing Configurations ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e23387",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove__slash__n = True\n",
    "lower__case = True\n",
    "remove__symbols = True\n",
    "remove__digits = True\n",
    "lemmatize__words = True\n",
    "remove__stopwords = True\n",
    "remove__special__characters = True\n",
    "removing_classes_that_has_only_one_member = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset PreProcessing #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63694e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountryNames = [\n",
    "    ('US', 'United States'),\n",
    "    ('AF', 'Afghanistan'),\n",
    "    ('AL', 'Albania'),\n",
    "    ('DZ', 'Algeria'),\n",
    "    ('AS', 'American Samoa'),\n",
    "    ('AD', 'Andorra'),\n",
    "    ('AO', 'Angola'),\n",
    "    ('AI', 'Anguilla'),\n",
    "    ('AQ', 'Antarctica'),\n",
    "    ('AG', 'Antigua And Barbuda'),\n",
    "    ('AR', 'Argentina'),\n",
    "    ('AM', 'Armenia'),\n",
    "    ('AW', 'Aruba'),\n",
    "    ('AU', 'Australia'),\n",
    "    ('AT', 'Austria'),\n",
    "    ('AZ', 'Azerbaijan'),\n",
    "    ('BS', 'Bahamas'),\n",
    "    ('BH', 'Bahrain'),\n",
    "    ('BD', 'Bangladesh'),\n",
    "    ('BB', 'Barbados'),\n",
    "    ('BY', 'Belarus'),\n",
    "    ('BE', 'Belgium'),\n",
    "    ('BZ', 'Belize'),\n",
    "    ('BJ', 'Benin'),\n",
    "    ('BM', 'Bermuda'),\n",
    "    ('BT', 'Bhutan'),\n",
    "    ('BO', 'Bolivia'),\n",
    "    ('BA', 'Bosnia And Herzegowina'),\n",
    "    ('BW', 'Botswana'),\n",
    "    ('BV', 'Bouvet Island'),\n",
    "    ('BR', 'Brazil'),\n",
    "    ('BN', 'Brunei Darussalam'),\n",
    "    ('BG', 'Bulgaria'),\n",
    "    ('BF', 'Burkina Faso'),\n",
    "    ('BI', 'Burundi'),\n",
    "    ('KH', 'Cambodia'),\n",
    "    ('CM', 'Cameroon'),\n",
    "    ('CA', 'Canada'),\n",
    "    ('CV', 'Cape Verde'),\n",
    "    ('KY', 'Cayman Islands'),\n",
    "    ('CF', 'Central African Rep'),\n",
    "    ('TD', 'Chad'),\n",
    "    ('CL', 'Chile'),\n",
    "    ('CN', 'China'),\n",
    "    ('CX', 'Christmas Island'),\n",
    "    ('CC', 'Cocos Islands'),\n",
    "    ('CO', 'Colombia'),\n",
    "    ('KM', 'Comoros'),\n",
    "    ('CG', 'Congo'),\n",
    "    ('CK', 'Cook Islands'),\n",
    "    ('CR', 'Costa Rica'),\n",
    "    ('CI', 'Cote D`ivoire'),\n",
    "    ('HR', 'Croatia'),\n",
    "    ('CU', 'Cuba'),\n",
    "    ('CY', 'Cyprus'),\n",
    "    ('CZ', 'Czech Republic'),\n",
    "    ('DK', 'Denmark'),\n",
    "    ('DJ', 'Djibouti'),\n",
    "    ('DM', 'Dominica'),\n",
    "    ('DO', 'Dominican Republic'),\n",
    "    ('TP', 'East Timor'),\n",
    "    ('EC', 'Ecuador'),\n",
    "    ('EG', 'Egypt'),\n",
    "    ('SV', 'El Salvador'),\n",
    "    ('GQ', 'Equatorial Guinea'),\n",
    "    ('ER', 'Eritrea'),\n",
    "    ('EE', 'Estonia'),\n",
    "    ('ET', 'Ethiopia'),\n",
    "    ('FK', 'Falkland Islands (Malvinas)'),\n",
    "    ('FO', 'Faroe Islands'),\n",
    "    ('FJ', 'Fiji'),\n",
    "    ('FI', 'Finland'),\n",
    "    ('FR', 'France'),\n",
    "    ('GF', 'French Guiana'),\n",
    "    ('PF', 'French Polynesia'),\n",
    "    ('TF', 'French S. Territories'),\n",
    "    ('GA', 'Gabon'),\n",
    "    ('GM', 'Gambia'),\n",
    "    ('GE', 'Georgia'),\n",
    "    ('DE', 'Germany'),\n",
    "    ('GH', 'Ghana'),\n",
    "    ('GI', 'Gibraltar'),\n",
    "    ('GR', 'Greece'),\n",
    "    ('GL', 'Greenland'),\n",
    "    ('GD', 'Grenada'),\n",
    "    ('GP', 'Guadeloupe'),\n",
    "    ('GU', 'Guam'),\n",
    "    ('GT', 'Guatemala'),\n",
    "    ('GN', 'Guinea'),\n",
    "    ('GW', 'Guinea-bissau'),\n",
    "    ('GY', 'Guyana'),\n",
    "    ('HT', 'Haiti'),\n",
    "    ('HN', 'Honduras'),\n",
    "    ('HK', 'Hong Kong'),\n",
    "    ('HU', 'Hungary'),\n",
    "    ('IS', 'Iceland'),\n",
    "    ('IN', 'India'),\n",
    "    ('ID', 'Indonesia'),\n",
    "    ('IR', 'Iran'),\n",
    "    ('IQ', 'Iraq'),\n",
    "    ('IE', 'Ireland'),\n",
    "    ('IL', 'Israel'),\n",
    "    ('IT', 'Italy'),\n",
    "    ('JM', 'Jamaica'),\n",
    "    ('JP', 'Japan'),\n",
    "    ('JO', 'Jordan'),\n",
    "    ('KZ', 'Kazakhstan'),\n",
    "    ('KE', 'Kenya'),\n",
    "    ('KI', 'Kiribati'),\n",
    "    ('KP', 'Korea (North)'),\n",
    "    ('KR', 'Korea (South)'),\n",
    "    ('KW', 'Kuwait'),\n",
    "    ('KG', 'Kyrgyzstan'),\n",
    "    ('LA', 'Laos'),\n",
    "    ('LV', 'Latvia'),\n",
    "    ('LB', 'Lebanon'),\n",
    "    ('LS', 'Lesotho'),\n",
    "    ('LR', 'Liberia'),\n",
    "    ('LY', 'Libya'),\n",
    "    ('LI', 'Liechtenstein'),\n",
    "    ('LT', 'Lithuania'),\n",
    "    ('LU', 'Luxembourg'),\n",
    "    ('MO', 'Macau'),\n",
    "    ('MK', 'Macedonia'),\n",
    "    ('MG', 'Madagascar'),\n",
    "    ('MW', 'Malawi'),\n",
    "    ('MY', 'Malaysia'),\n",
    "    ('MV', 'Maldives'),\n",
    "    ('ML', 'Mali'),\n",
    "    ('MT', 'Malta'),\n",
    "    ('MH', 'Marshall Islands'),\n",
    "    ('MQ', 'Martinique'),\n",
    "    ('MR', 'Mauritania'),\n",
    "    ('MU', 'Mauritius'),\n",
    "    ('YT', 'Mayotte'),\n",
    "    ('MX', 'Mexico'),\n",
    "    ('FM', 'Micronesia'),\n",
    "    ('MD', 'Moldova'),\n",
    "    ('MC', 'Monaco'),\n",
    "    ('MN', 'Mongolia'),\n",
    "    ('MS', 'Montserrat'),\n",
    "    ('MA', 'Morocco'),\n",
    "    ('MZ', 'Mozambique'),\n",
    "    ('MM', 'Myanmar'),\n",
    "    ('NA', 'Namibia'),\n",
    "    ('NR', 'Nauru'),\n",
    "    ('NP', 'Nepal'),\n",
    "    ('NL', 'Netherlands'),\n",
    "    ('AN', 'Netherlands Antilles'),\n",
    "    ('NC', 'New Caledonia'),\n",
    "    ('NZ', 'New Zealand'),\n",
    "    ('NI', 'Nicaragua'),\n",
    "    ('NE', 'Niger'),\n",
    "    ('NG', 'Nigeria'),\n",
    "    ('NU', 'Niue'),\n",
    "    ('NF', 'Norfolk Island'),\n",
    "    ('MP', 'Northern Mariana Islands'),\n",
    "    ('NO', 'Norway'),\n",
    "    ('OM', 'Oman'),\n",
    "    ('PK', 'Pakistan'),\n",
    "    ('PW', 'Palau'),\n",
    "    ('PA', 'Panama'),\n",
    "    ('PG', 'Papua New Guinea'),\n",
    "    ('PY', 'Paraguay'),\n",
    "    ('PE', 'Peru'),\n",
    "    ('PH', 'Philippines'),\n",
    "    ('PN', 'Pitcairn'),\n",
    "    ('PL', 'Poland'),\n",
    "    ('PT', 'Portugal'),\n",
    "    ('PR', 'Puerto Rico'),\n",
    "    ('QA', 'Qatar'),\n",
    "    ('RE', 'Reunion'),\n",
    "    ('RO', 'Romania'),\n",
    "    ('RU', 'Russian Federation'),\n",
    "    ('RW', 'Rwanda'),\n",
    "    ('KN', 'Saint Kitts And Nevis'),\n",
    "    ('LC', 'Saint Lucia'),\n",
    "    ('VC', 'St Vincent/Grenadines'),\n",
    "    ('WS', 'Samoa'),\n",
    "    ('SM', 'San Marino'),\n",
    "    ('ST', 'Sao Tome'),\n",
    "    ('SA', 'Saudi Arabia'),\n",
    "    ('SN', 'Senegal'),\n",
    "    ('SC', 'Seychelles'),\n",
    "    ('SL', 'Sierra Leone'),\n",
    "    ('SG', 'Singapore'),\n",
    "    ('SK', 'Slovakia'),\n",
    "    ('SI', 'Slovenia'),\n",
    "    ('SB', 'Solomon Islands'),\n",
    "    ('SO', 'Somalia'),\n",
    "    ('ZA', 'South Africa'),\n",
    "    ('ES', 'Spain'),\n",
    "    ('LK', 'Sri Lanka'),\n",
    "    ('SH', 'St. Helena'),\n",
    "    ('PM', 'St.Pierre'),\n",
    "    ('SD', 'Sudan'),\n",
    "    ('SR', 'Suriname'),\n",
    "    ('SZ', 'Swaziland'),\n",
    "    ('SE', 'Sweden'),\n",
    "    ('CH', 'Switzerland'),\n",
    "    ('SY', 'Syrian Arab Republic'),\n",
    "    ('TW', 'Taiwan'),\n",
    "    ('TJ', 'Tajikistan'),\n",
    "    ('TZ', 'Tanzania'),\n",
    "    ('TH', 'Thailand'),\n",
    "    ('TG', 'Togo'),\n",
    "    ('TK', 'Tokelau'),\n",
    "    ('TO', 'Tonga'),\n",
    "    ('TT', 'Trinidad And Tobago'),\n",
    "    ('TN', 'Tunisia'),\n",
    "    ('TR', 'Turkey'),\n",
    "    ('TM', 'Turkmenistan'),\n",
    "    ('TV', 'Tuvalu'),\n",
    "    ('UG', 'Uganda'),\n",
    "    ('UA', 'Ukraine'),\n",
    "    ('AE', 'United Arab Emirates'),\n",
    "    ('UK', 'United Kingdom'),\n",
    "    ('UY', 'Uruguay'),\n",
    "    ('UZ', 'Uzbekistan'),\n",
    "    ('VU', 'Vanuatu'),\n",
    "    ('VA', 'Vatican City State'),\n",
    "    ('VE', 'Venezuela'),\n",
    "    ('VN', 'Viet Nam'),\n",
    "    ('VG', 'Virgin Islands (British)'),\n",
    "    ('VI', 'Virgin Islands (U.S.)'),\n",
    "    ('EH', 'Western Sahara'),\n",
    "    ('YE', 'Yemen'),\n",
    "    ('YU', 'Yugoslavia'),\n",
    "    ('ZR', 'Zaire'),\n",
    "    ('ZM', 'Zambia'),\n",
    "    ('ZW', 'Zimbabwe'),\n",
    "    ('Great Britain', \"Britain\"),\n",
    "    ('USA', \"America\"),\n",
    "    ('RU', 'Russia')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CountryItems=[item for t in CountryNames for item in t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b42021",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(CountryItems)):\n",
    "    CountryItems[i]=CountryItems[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270113be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions Pre-Processing the Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2024e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing \\n from speeches\n",
    "def removing_slash_n():\n",
    "    if not remove__slash__n:\n",
    "        return\n",
    "    for row_number in trange(df.shape[0], desc=\"Removing '\\\\n'\"):\n",
    "        df.iloc[row_number, 4] = df.iloc[row_number, 4].replace('\\n', ' ')\n",
    "\n",
    "# lowercasing all the words in the speeches\n",
    "def lowering_letters():\n",
    "    if not lower__case:\n",
    "        return\n",
    "    for row_number in trange(df.shape[0], desc='Lower Cases'):\n",
    "        df.iloc[row_number, 4] = df.iloc[row_number, 4].lower()\n",
    "\n",
    "# removing symbols using regex\n",
    "def removing_symbols():\n",
    "    if not remove__symbols:\n",
    "        return\n",
    "    for row_number in trange(df.shape[0], desc = 'Removing Symbols'):\n",
    "        speech = df.iloc[row_number, 4]\n",
    "        speech_tokens = word_tokenize(speech)\n",
    "        clean = []\n",
    "        for token in speech_tokens:\n",
    "            res = re.sub(r'[^\\w\\s]',\" \", token)\n",
    "            if res != '':\n",
    "                clean.append(res)\n",
    "        df.iloc[row_number, 4] = ' '.join(clean)\n",
    "\n",
    "#removing numerical values\n",
    "def only_is_alpha():\n",
    "    if not remove__digits:\n",
    "        return\n",
    "    for row_number in trange(df.shape[0], desc = 'Keeping only Alphabetical characters'):\n",
    "        speech = df.iloc[row_number, 4]\n",
    "        speech_tokens = word_tokenize(speech)\n",
    "        df.iloc[row_number, 4] = ' '.join([word for word in speech_tokens if word.isalpha()])\n",
    "\n",
    "#removing special characters\n",
    "def removing_special_characters():\n",
    "    if not remove__special__characters:\n",
    "        return\n",
    "\n",
    "    def contains_special_characters(word : str) -> bool:\n",
    "        for char in word:\n",
    "            if not ('a' <= char <= 'z'):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    for row_number in trange(df.shape[0], desc='Removing Special Characters'):\n",
    "        speech = df.iloc[row_number, 4]\n",
    "        speech_tokens = word_tokenize(speech)\n",
    "        df.iloc[row_number, 4] = ' '.join([w for w in speech_tokens if not contains_special_characters(w)])\n",
    "\n",
    "#lemmatizing the words\n",
    "def lemmatizing_speeches():\n",
    "    if not lemmatize__words:\n",
    "        return\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for row_number in trange(df.shape[0], desc='Lemmatizing Speeches'):\n",
    "        speech = df.iloc[row_number, 4]\n",
    "        speech_tokens = word_tokenize(speech)\n",
    "        df.iloc[row_number, 4] = ' '.join([lemmatizer.lemmatize(word) for word in speech_tokens])\n",
    "\n",
    "#removing stopwords\n",
    "def removing_stopwords():\n",
    "    if not remove__stopwords:\n",
    "        return\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(CountryItems)\n",
    "    \n",
    "    for row_number in trange(df.shape[0], desc='Removing StopWords'):\n",
    "        speech = df.iloc[row_number, 4]\n",
    "        speech_tokens = word_tokenize(speech)\n",
    "        df.iloc[row_number, 4] = ' '.join([w for w in speech_tokens if w not in stop_words and len(w) > 2 and w != 'must'])\n",
    "\n",
    "#removing words whose POS is noun\n",
    "def removing_nouns():\n",
    "    for row_number in trange(df.shape[0], desc='Removing NOUN'):\n",
    "        sentences = sent_tokenize(df.iloc[row_number, 4])\n",
    "        for i in range(len(sentences)):\n",
    "            sentence=sentences[i]\n",
    "            tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "            edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS' and tag != 'PRP' and  tag != 'WP' and tag != 'WDT' and tag != 'PRT' and tag!='TO']\n",
    "            sentences[i]=' '.join(edited_sentence)\n",
    "        df.iloc[row_number, 4] = ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data PreProcessing Shot Callers ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "removing_slash_n()\n",
    "lowering_letters()\n",
    "removing_symbols()\n",
    "only_is_alpha() \n",
    "removing_special_characters()\n",
    "lemmatizing_speeches()\n",
    "removing_stopwords()\n",
    "removing_nouns()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to Gather Insights on Dataset PreProcessing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique characters\n",
    "def find_unique_characters():\n",
    "    unique_characters = []\n",
    "    for speech in tqdm(df['Text_of_Speech'], desc='Finding Unique Characters'):\n",
    "        for char in speech:\n",
    "            if char.lower() not in unique_characters:\n",
    "                unique_characters.append(char.lower())\n",
    "    unique_characters.sort()\n",
    "    print(unique_characters)\n",
    "find_unique_characters()\n",
    "\n",
    "def find_alpha_numeric_mix():\n",
    "    mix = []\n",
    "    for speech in tqdm(df['Text_of_Speech'], desc='Finding Alpha Numeric Mix'):\n",
    "        words_splitted = speech.split()\n",
    "        for word in words_splitted:\n",
    "            letter = False\n",
    "            num = False\n",
    "            for char in word:\n",
    "                if 'a' <= char <= 'z':\n",
    "                    letter = True\n",
    "                if '0' <= char <= '9':\n",
    "                    num = True\n",
    "            if letter and num:\n",
    "                mix.append(word)\n",
    "    print(mix)\n",
    "find_alpha_numeric_mix()\n",
    "\n",
    "def find_words_with_special_characters():\n",
    "    words_with_special_characters = []\n",
    "    for speech in tqdm(df['Text_of_Speech']):\n",
    "        words = speech.split()\n",
    "        for word in words:\n",
    "            for char in word:\n",
    "                if not ('a' <= char <= 'z'):\n",
    "                    words_with_special_characters.append(word)\n",
    "    print(words_with_special_characters)\n",
    "find_words_with_special_characters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6baf1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_speech_lengths = [len(word_tokenize(speech)) for speech in df['Text_of_Speech']]\n",
    "plt.hist(pre_processed_speech_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec56cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_speech_lengths.sort(reverse = True)\n",
    "pre_processed_speech_lengths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c720ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if removing_classes_that_has_only_one_member:\n",
    "    for target__variable in list(df)[1:]:\n",
    "        classes = list(df[target__variable].unique())\n",
    "        targets = list(df[target__variable])\n",
    "        df_filtered = df\n",
    "        for each_class in classes:\n",
    "            freq = targets.count(each_class)\n",
    "            if freq <= 1:\n",
    "                df_filtered = df_filtered[df_filtered[target__variable] != each_class]\n",
    "        df = df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e95fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for column_name in list(df):\n",
    "    data[column_name] = list(df[column_name])\n",
    "df_final = pd.DataFrame(data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b5a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('Dataset_PreProcessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
